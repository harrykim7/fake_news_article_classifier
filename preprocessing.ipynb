{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " #if you already downloaded it, comment it\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#congfig things\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = set([w.lower() for w in nltk.corpus.words.words()])\n",
    "spell = Speller(lang='en')\n",
    "regex = re.compile('[^a-zA-Z0-9 ]')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "min_len = 3\n",
    "max_len = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "xy_train_df = pd.read_csv('news_dataset.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train_df = xy_train_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional preprocessing beside vocab things\n",
    "def all_preprocessing(df,remove_stop=True,word_stem=True):\n",
    "    temp = df.copy()\n",
    "    for i in tqdm(range(len(temp))):\n",
    "        text = regex.sub(\"\",temp.loc[i].content).lower() #remove all non-numeric.space,non-alpha and ^ thing\n",
    "        text_tokens = word_tokenize(text)\n",
    "        if len(text_tokens) > max_len or len(text_tokens) < min_len: # if a text have less then min_len or more than max_len drop it\n",
    "            temp.drop([i],inplace=True)\n",
    "        else:\n",
    "            #spell correcting\n",
    "            for token_idx in range(len(text_tokens)):\n",
    "                token = text_tokens[token_idx]\n",
    "                if token not in words:\n",
    "                    spell_check = spell(token)\n",
    "                    token = spell_check\n",
    "                    text_tokens[token_idx] = spell_check\n",
    "            #remove stop words\n",
    "            if remove_stop:\n",
    "                text_tokens = [w for w in text_tokens if not w in stop_words]\n",
    "            #word stemming\n",
    "            if word_stem:\n",
    "                text_tokens = [ps.stem(w) for w in text_tokens]\n",
    "            temp.at[i,\"content\"] = \" \".join(text_tokens)[0:]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional preprocessing beside vocab things\n",
    "def all_preprocessing_title(df,remove_stop=True,word_stem=True):\n",
    "    temp = df.copy()\n",
    "    for i in tqdm(range(len(temp))):\n",
    "        text = regex.sub(\"\",temp.loc[i].title).lower() #remove all non-numeric.space,non-alpha and ^ thing\n",
    "        text_tokens = word_tokenize(text)\n",
    "        if len(text_tokens) > max_len or len(text_tokens) < min_len: # if a text have less then min_len or more than max_len drop it\n",
    "            temp.drop([i],inplace=True)\n",
    "        else:\n",
    "            #spell correcting\n",
    "            for token_idx in range(len(text_tokens)):\n",
    "                token = text_tokens[token_idx]\n",
    "                if token not in words:\n",
    "                    spell_check = spell(token)\n",
    "                    token = spell_check\n",
    "                    text_tokens[token_idx] = spell_check\n",
    "            #remove stop words\n",
    "            if remove_stop:\n",
    "                text_tokens = [w for w in text_tokens if not w in stop_words]\n",
    "            #word stemming\n",
    "            if word_stem:\n",
    "                text_tokens = [ps.stem(w) for w in text_tokens]\n",
    "            temp.at[i,\"title\"] = \" \".join(text_tokens)[0:]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/27985 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'max_len' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6722f1c7ef73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxy_train_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy_train_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-0de6f670c8b0>\u001b[0m in \u001b[0;36mall_preprocessing\u001b[1;34m(df, remove_stop, word_stem)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remove all non-numeric.space,non-alpha and ^ thing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_tokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_tokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# if a text have less then min_len or more than max_len drop it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_len' is not defined"
     ]
    }
   ],
   "source": [
    "xy_train_preprocessed = all_preprocessing(xy_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20706"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xy_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the preprocess time is so long, save the result incase of something bad happens\n",
    "xy_train_preprocessed.to_csv(\"news_dataset_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train_preprocessed = xy_train_preprocessed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20706/20706 [11:13<00:00, 30.74it/s]\n"
     ]
    }
   ],
   "source": [
    "xy_train_preprocessed_all = all_preprocessing_title(xy_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the preprocess time is so long, save the result incase of something bad happens\n",
    "xy_train_preprocessed_all.to_csv(\"news_dataset_preprocessed_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20408"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xy_train_preprocessed_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}